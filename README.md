
# 基于Transformer的中文文本生成模型

## 项目概述

本项目实现了一个基于PyTorch的Transformer语言模型，专注于中文文本的自回归生成任务。模型采用《红楼梦》作为训练语料库，通过深度学习技术学习古典文学的语言模式和文体特征，实现文本续写和生成功能。

**核心技术特性：**
- **分布式并行计算**：支持多GPU数据并行训练
- **训练监控系统**：集成自定义监控系统，提供训练指标追踪
- **结构化日志系统**：基于loguru实现的日志管理和异常追踪机制
- **模型架构优化**：针对中文语言特性优化的Transformer实现
- **检查点管理**：自动检查点保存与恢复功能
- **训练可视化**：自动生成损失曲线和训练指标图表

本项目提供了一个完整的深度学习研究平台，适用于自然语言处理和Transformer架构的学术研究与工程实践。

## 核心特性

### 模型架构
- **Transformer编码器栈**：12层深度Transformer块，采用768维隐藏状态和12头多头注意力机制
- **分布式计算架构**：支持单节点多GPU数据并行训练
- **学习率调度**：余弦退火学习率衰减策略，集成线性预热机制

### 训练监控与分析
- **指标监控**：追踪训练/验证损失、学习率、GPU内存占用和计算吞吐量
- **可视化功能**：基于matplotlib的训练曲线生成
- **数据持久化**：采用JSON格式存储训练元数据
- **进度追踪**：集成tqdm进度条，提供训练状态反馈

### 软件工程特性
- **模块化架构**：采用组件化设计模式，支持功能模块的独立开发
- **配置管理**：实现集中化的超参数管理
- **异常处理**：异常捕获、日志记录和错误恢复机制
- **检查点管理**：自动化的模型状态保存与恢复功能

## 项目结构

```
dive-into-transformer-pytorch/
├── .gitignore                # Git忽略文件配置
├── README.md                 # 项目说明文档
├── requirements.txt          # Python依赖包列表
├── data/                     # 数据目录
│   ├── raw/                  # 原始训练数据
│   │   └── Hong_Lou_Meng.txt # 红楼梦文本数据
│   ├── processed/            # 预处理后的数据
│   └── splits/               # 数据集划分
├── src/                      # 源代码目录
│   ├── config.py             # 配置管理（超参数、分布式配置）
│   ├── main.py               # 主训练脚本
│   ├── model.py              # Transformer模型定义
│   ├── utils.py              # 工具函数（数据处理、评估等）
│   ├── monitor.py            # 训练监控和可视化
│   └── logger.py             # 日志配置
├── logs/                     # 训练日志和输出
│   ├── plots/                # 训练曲线图
│   ├── training_*.log        # 详细训练日志
│   └── training_metrics.json # 训练指标数据
└── checkpoints/              # 模型检查点
    └── checkpoint_iter_*.pt  # 训练检查点文件
```

## 环境配置

### 系统要求
- **Python**: 3.8+
- **CUDA**: 11.8+ (GPU训练)
- **内存**: 16GB+ 推荐
- **GPU**: NVIDIA RTX 3090/4090 或更高（支持多GPU）

### 安装步骤

1. **克隆项目**
```bash
git clone https://github.com/Excelius-Wang/dive-into-transformer-pytorch
cd dive-into-transformer-pytorch
```

2. **安装依赖**
```bash
pip install -r requirements.txt
```

3. **数据准备**
将训练文本放入 `data/raw/Hong_Lou_Meng.txt`，或修改 `config.py` 中的 `data_file` 路径。

## 使用指南

### 单GPU训练
```bash
python src/main.py
```

### 分布式训练（推荐）

**双GPU训练：**
```bash
python -m torch.distributed.launch --nproc_per_node=2 --master_port=29500 src/main.py
```

**多节点训练：**
```bash
# 主节点
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=192.168.1.100 --master_port=29500 src/main.py

# 从节点
torchrun --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=192.168.1.100 --master_port=29500 src/main.py
```

### 训练监控

训练过程中会自动：
- 生成训练曲线图保存到 `logs/plots/`
- 记录详细日志到 `logs/training_*.log`
- 保存检查点到 `checkpoints/`
- 实时显示训练进度和指标

### 文本生成推理

模型训练完成后将自动执行文本生成任务并输出样本结果。可通过修改 `config.py` 中的推理参数进行定制：

```python
max_new_tokens = 500  # 最大生成序列长度
block_size = 256      # 上下文窗口大小(序列长度)
```

## 技术架构

### 模型架构
```
GPTLanguageModel(
  (token_embedding_table): Embedding(vocab_size, 768)
  (position_embedding_table): Embedding(256, 768)
  (blocks): ModuleList(
    (0-11): 12 x Block(
      (sa): MultiHeadAttention(
        (heads): ModuleList(0-11): 12 x Head(64)
        (proj): Linear(768, 768)
        (dropout): Dropout(0.1)
      )
      (ffwd): FeedForward(
        (net): Sequential(
          (0): Linear(768, 3072)
          (1): ReLU()
          (2): Linear(3072, 768)
          (3): Dropout(0.1)
        )
      )
      (ln1): LayerNorm(768)
      (ln2): LayerNorm(768)
    )
  )
  (ln_f): LayerNorm(768)
  (lm_head): Linear(768, vocab_size)
)
```

### 核心参数
| 参数 | 值 | 说明 |
|------|----|---------|
| 嵌入维度 | 768 | 词向量和位置编码维度 |
| 注意力头数 | 12 | 多头自注意力机制 |
| Transformer层数 | 12 | 深度网络层数 |
| 上下文长度 | 256 | 最大序列长度 |
| 词汇表大小 | ~4000 | 中文字符词汇表 |
| 参数总量 | ~91M | 模型总参数数量 |

### 训练超参数配置
- **优化算法**: AdamW优化器 (学习率=1e-4, 权重衰减=0.1)
- **学习率调度策略**: 余弦退火衰减 + 线性预热(500步)
- **批处理大小**: 16 (可根据GPU内存容量动态调整)
- **梯度裁剪阈值**: 1.0 (防止梯度爆炸)
- **正则化**: Dropout概率=0.1

## 性能表现

### 模型性能指标
- **训练损失收敛**：从初始损失收敛至较低水平
- **泛化性能**：验证集损失表现稳定
- **计算吞吐量**：双RTX 3090配置下可达到合理的处理速度
- **收敛效率**：模型在合理的迭代次数内达到收敛

### 生成样例

**输入上下文**：
> 我就好回去了。只是一件，我死也不甘心，我虽生得比别人好些，并没有私情勾引你，怎么一口死咬定了我是个‘狐狸精’！我今儿既担了虚名，况且没了远限，不是我说一句后悔的话，早知如此，我当日……”说到这里，气往上咽，便说不出来，两手已经冰凉。宝玉又痛又急，又害怕，便歪在席上，一只手摸着他的手，一只手轻轻的给他捶打着。又不敢大声的叫，真真万箭攒心。两三句话时晴雯才哭出来，宝玉拉着他的手，只觉瘦如枯柴。腕上犹戴着四个银镯，因哭道：“除下来，等好了再戴上去罢。”又说：“这一病好了，又瘦好些！”晴雯拭泪，把那手用力拳回，搁在

**AI续写**：
> 我就好回去了。只是一件，我死也不甘心，我虽生得比别人好些，并没有私情勾引你，怎么一口死咬定了我是个‘狐狸精’！我今儿既担了虚名，况且没了远限，不是我说一句后悔的话，早知如此，我当日……”说到这里，气往上咽，便说不出来，两手已经冰凉。宝玉又痛又急，又害怕，便歪在席上，一只手摸着他的手，一只手轻轻的给他捶打着。又不敢大声的叫，真真万箭攒心。两三句话时晴雯才哭出来，宝玉拉着他的手，只觉瘦如枯柴。腕上犹戴着四个银镯，因哭道：“除下来，等好了再戴上去罢。”又说：“这一病好了，又瘦好些！”晴雯拭泪，把那手用力拳回，搁在**伸手场内。更望着要礁，出气死活一鼻有岭事，不再问眼花咚一头家子殄天的芸，自然便一壁，瞅了他葬纳闷。宝玉也觉不起。此时正有日吹了他一几遍，却又怕荣无人，起贾雨了急，出入园中人来。恐开了甸掉过来，便开了权往外边。只见青天亮疯话，恐圣精神，娇懒怠放，八六夜系了脚，从锦乡之姿色。转过碗，底下的日五鼓，白日更至功，不作巧。待要上拱穗各处众官，每日衣音遗漏溪御前来，盘乞蒸大院，然后炕下添添乃是焦坠一般。贾政不能，悬着堂房尸接，憨耳朵木板，烦猛抬头魂，隐寒温水，连忙掩了波了龟。早又拉着宝玉道：“我可来了！”方说着，只见贾母盗三让他进去。他漱了半日，方才出来，由不得敝友，一直坐在新塞着，便簠坐下，方开戏酒席，替他踉煌。只见榻上放下擦脸。探春淋得孙采了些皮褂子，也不永头飞歹，起来笑身取，挤起园来。他叔嫂的茶果，坐在一处，深喜鸾癖才按住。宝玉亦思自懂了，呢胎添补心绛盒内各色洞，真是淡光而已。贾赦先有贾政一会，便指起字：“不去，未必进益，才不念我幌，花儿捞出罪就来。”于是贾政了邢夫人，出过来跪着贾母住，点的两张遗纸，宝玉默默宽董，贾芸绽书旦的已望一笞，苦婳不曾。后去后又蹲在贾赦、贾政处。宝玉又问贾宅相**

### 分布式训练性能
支持多GPU并行训练，可根据硬件配置获得相应的性能提升。具体性能指标取决于硬件配置和训练参数设置。

## 开发指南

### 自定义配置
修改 `src/config.py` 中的参数：
```python
class Config:
    # 模型架构
    n_embd = 768        # 嵌入维度
    n_head = 12         # 注意力头数
    n_layer = 12        # 层数

    # 训练参数
    batch_size = 16     # 批次大小
    learning_rate = 1e-4 # 学习率
    max_iters = 5000    # 训练轮数
```

### 功能扩展
1. **数据集适配**: 修改 `utils.py` 中的 `load_and_process_data()` 函数以支持新的数据格式
2. **模型架构修改**: 在 `model.py` 中扩展 `GPTLanguageModel` 类，实现新的网络层或注意力机制
3. **训练算法调整**: 在 `main.py` 中调整训练循环，集成新的优化策略或损失函数
4. **监控指标扩展**: 在 `monitor.py` 中添加自定义的性能评估指标和可视化组件

### 故障排除

**常见问题解决**：
- **CUDA内存溢出**：调整`batch_size`或`block_size`参数以适配GPU内存容量
- **分布式通信故障**：验证网络端口可用性和进程间通信配置
- **数据序列化异常**：确保训练指标为Python原生数据类型，避免Tensor对象序列化
- **检查点恢复失败**：验证文件系统权限和路径有效性

## 许可证

本项目采用MIT许可证，详见 [LICENSE](LICENSE) 文件。

---

如果这个项目对您有帮助，欢迎给予Star支持。
