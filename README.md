
# 🤖 红楼梦AI续写大师

## 📝 项目简介

嘿！这是我的小小实验室，一个能模仿《红楼梦》文风自动续写的AI模型！身为研究生，熬夜肝代码期间突发奇想：要是能让AI帮我写红学论文素材该多好啊！于是这个项目就诞生了~ 😂

本质上是一个基于Transformer架构的中文语言模型，能够学习中文文本风格后进行自动续写。用《红楼梦》训练出来的模型，生成的文本还挺有意思的，有时候连我自己都分不清是曹雪芹写的还是AI写的！

## ✨ 项目特点

- **会说中文**：专门为中文文本优化，使用jieba分词（不是"街霸"！😆）
- **头真的很多**：多头自注意力机制，就像考试时能同时关注试卷和参考书一样
- **多GPU并行**：训练速度嗖嗖的，让我的破显卡不再孤单
- **模块化设计**：代码结构清晰，连半夜被导师叫醒也能迅速定位问题
- **交互式生成**：可以给它一个开头，让它续写，比如"王熙凤道："，看它能编出什么故事

## 🏗️ 项目结构

```
dive-into-transformer-pytorch/
├── data/                     # 数据躺这儿
│   └── raw/                  # 原始数据
│       └── Hong_Lou_Meng.txt # 曹公巨著（AI的精神食粮）
├── src/                      # 源代码
│   ├── config/               # 配置相关（参数都在这调）
│   ├── data_loader/          # 数据加载
│   ├── models/               # 模型定义（重点看这里）
│   ├── training/             # 训练相关
│   ├── generate/             # 文本生成
│   ├── utils/                # 工具函数
│   └── main.py               # 主程序（运行它就对了）
└── README.md                 # 就是你现在看的这个
```

## 🔧 安装步骤

### 前提条件
* 一台电脑
* 最好有GPU（没有的话去蹭实验室的）
* Python环境
* 耐心（训练需要时间）

1. 克隆仓库（复制到你的电脑上）
```bash
git clone https://github.com/yourusername/dive-into-transformer-pytorch.git
cd dive-into-transformer-pytorch
```

2. 安装依赖（让电脑认识需要用的工具）
```bash
pip install -r requirements.txt
```

主要依赖：
- torch (深度学习界的瑞士军刀)
- jieba (中文分词小能手)
- numpy (矩阵计算老大哥)

3. 准备数据（放进去等待消化）
把《红楼梦》文本文件放到`data/raw/`目录下，也可以换成其他中文小说，比如《三体》（不过生成的内容可能会有宇宙黑暗森林警告😱）

## 🚀 使用方法

### 训练模型
```bash
python src/main.py --train
```
然后就可以去喝奶茶了，回来说不定就训练好了~

### 生成文本
从随机段落开始续写：
```bash
python src/main.py --generate
```

接续你自己写的开头：
```bash
python src/main.py --generate --context "贾宝玉看着林黛玉道："
```

想让AI多说几句：
```bash
python src/main.py --generate --max_tokens 500
```

## 🔬 技术细节

- **词嵌入**：把文字变成电脑能理解的向量（想象成把每个词变成一个多维空间中的小点）
- **注意力机制**：让模型知道该关注哪些词（就像考试时你知道哪些知识点最重要）
- **Transformer块**：整个模型的核心，包含多头自注意力和前馈网络
- **多层结构**：像叠千层饼一样，一层一层提取文本特征
- **技术参数**：
  - 嵌入维度：512（论文里说的，信就对了）
  - 注意力头数：8个（多多益善？）
  - Transformer层数：12层（GPT-2同款配置）
  - 上下文窗口：256个词（够《红楼梦》表达一个小场景了）

## 📝 样例输出

> **上文**：贾宝玉看着林黛玉道："你今日为何不开心？可是昨日..."
> 
> **AI续写**：...会芳园的姐妹们又说了什么闲话？"林黛玉垂泪道："你问这些作甚，左不过是人人都嫌我多心罢了。"宝玉慌忙上前拉住黛玉的手道："我的林妹妹，你怎么又多心了，谁敢说你什么..."

## 💬 碎碎念

这个项目是我研一期间的课余实验，代码可能还有很多可以优化的地方。训练时看着loss一点点下降，就像看着自己养的数字宠物一点点成长，莫名有成就感！

希望能帮到同样对NLP感兴趣的同学，有任何问题欢迎交流，我不一定能解答，但一定会装作很懂的样子回复你！🤣

---

*PS: 如果我的导师看到这个项目，麻烦装作不知道我用实验室算力训练了这个模型，谢谢！*
